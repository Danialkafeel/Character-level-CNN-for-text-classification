{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMAI_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gWuenkx4JyG"
      },
      "source": [
        "**In this project we implement the paper :**\n",
        "\n",
        "# Character-level Convolutional Networks for Text Classification\n",
        "Published by ``Xiang Zhang, Junbo Zhao, Yann LeCun``\n",
        "\n",
        "- We have used AGâ€™s news corpus dataset to show that character-level convolutional networks could achieve\n",
        "state-of-the-art or competitive results.\n",
        "\n",
        "- The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. \n",
        "    - Each class contains 30,000 training samples and 1,900 testing samples. \n",
        "    - The total number of training samples is 120,000 and testing 7,600."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao2RQ9LDKs7t"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgw-82dZHNcB"
      },
      "source": [
        "Importing the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-M-Hq56K6Hb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egj01TAoLEm8"
      },
      "source": [
        "Loading the AG's News corpurs dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOx6EtlP1Nil",
        "outputId": "c0c5bf41-2005-4aed-9ee3-eb0db87e02fd"
      },
      "source": [
        "# path to our dataset\n",
        "train_path = '/content/train.csv'\n",
        "test_path = '/content/test.csv'\n",
        "\n",
        "# loading dataset in pandas dataframe\n",
        "train_df = pd.read_csv(train_path, header=None)\n",
        "test_df = pd.read_csv(test_path, header=None)\n",
        "\n",
        "print(\"Shape of the Train Data : \", train_df.shape)\n",
        "print(\"Shape of the Test Data : \", test_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the Train Data :  (120000, 3)\n",
            "Shape of the Test Data :  (7600, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGV-uUXaLhnP"
      },
      "source": [
        "Since the text of both train & test data spans across 2 columns so we :\n",
        "1. Concatenate column 2 and column 3 of the given Train & Test dataset\n",
        "2. Then drop the redundant column 3 in both Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvH0d7m0LMWh",
        "outputId": "cd14c6ff-a76e-4041-cd6c-88adc1ae3d78"
      },
      "source": [
        "# concatenating column 2 and 3 of train\n",
        "train_df[1] = train_df[1] + train_df[2]\n",
        "# dropping redundant column 3 in train\n",
        "train_df = train_df.drop([2],axis=1)\n",
        "\n",
        "# concatenating column 2 and 3 of test\n",
        "test_df[1] = test_df[1] + test_df[2]\n",
        "# dropping redundant column 3 in test\n",
        "test_df = test_df.drop([2],axis=1)\n",
        "\n",
        "print(\"Shape of the Train Data : \", train_df.shape)\n",
        "print(\"Shape of the Test Data : \", test_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the Train Data :  (120000, 2)\n",
            "Shape of the Test Data :  (7600, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsQ1Shh8MyXK"
      },
      "source": [
        "Now since the characters in our alphabet are all smallcase letters so we will now convert the train & test text to all smallcase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "verJTSzc6hYZ"
      },
      "source": [
        "# converting train data to lowercase\n",
        "train_lower = train_df[1].str.lower()\n",
        "\n",
        "# converting test data to lowercase\n",
        "test_lower = test_df[1].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYpl9C5vNsA1"
      },
      "source": [
        "In this section we will tokenize the text in the training dataset\n",
        "- We set the `char_level=True` so the we can process the text character by character\n",
        "- Next we set `oov_token=UNK` to add this token to vocabulary in order to handle unseen chracters in the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4UlxIVIsfbc"
      },
      "source": [
        "# initializing the tokenizer\n",
        "tokens = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "# fitting tokenizer on the training data\n",
        "tokens.fit_on_texts(train_lower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKT81mjzPK4D"
      },
      "source": [
        "We will now define the characters in our alphabet. And then map the characters to integers in range `[1, len(alphabet)]` in the same order as they appear in the alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ef4-3iPBfx"
      },
      "source": [
        "# known characters in our alphabet\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "\n",
        "# mapping of character to integer\n",
        "character_dictionary = dict()\n",
        "for i in range(len(alphabet)):\n",
        "    character_dictionary[alphabet[i]] = i + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12xZ-hRpP3Ze"
      },
      "source": [
        "- Then we set the word_index of tokenizer to the character mapping we just mapped. Also we map the UNK token to a value not in our dictionary, here `len(alphabet) + 1`\n",
        "\n",
        "- Then we convert the lowercase test & train data to sequence of integers using our dictionary mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krDvLf-5P2dZ"
      },
      "source": [
        "tokens.word_index = character_dictionary \n",
        "tokens.word_index[tokens.oov_token] = len(alphabet) + 1\n",
        "\n",
        "# converting data to sequence of integers using our character to int mapping\n",
        "train_sequences = tokens.texts_to_sequences(train_lower)\n",
        "test_sequences = tokens.texts_to_sequences(test_lower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9LtoK2QSgc"
      },
      "source": [
        "Next we add padding to the sequences so that they are all of the same max size of 1014"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prHuasTmQcCq"
      },
      "source": [
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=1014)\n",
        "test_padded = pad_sequences(test_sequences,  padding='post', maxlen=1014)\n",
        "\n",
        "train_data = np.array(train_padded)\n",
        "test_data = np.array(test_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtYVnhAsRLsR"
      },
      "source": [
        "Now we will find the available labels/classes of both the train & test text data. Classes are `0,1,2 and 3`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCowfZPKRYJB"
      },
      "source": [
        "# classes corresponding to the train data\n",
        "train_classes = np.array(train_df[0].values)\n",
        "train_classes = train_classes - 1\n",
        "\n",
        "# classes corresponding to the test data\n",
        "test_classes = np.array(test_df[0].values)\n",
        "test_classes = test_classes - 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BcjyaXpSMVM"
      },
      "source": [
        "In the final step of the data preprocessing \n",
        "\n",
        "we will use the `to_categorical` API in keras to convert array of our class/label data to one-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vMM_65-SMjX"
      },
      "source": [
        "train_classes = to_categorical(train_classes)\n",
        "test_classes = to_categorical(test_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_RvFzkTAoB"
      },
      "source": [
        "# Debugging (to be deleted before submission)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QdNYziL1vTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c61a7e-5403-44ab-d934-c881eb8bd3b9"
      },
      "source": [
        "train_data.shape, test_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120000, 1014), (7600, 1014))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqoba5si4nL8",
        "outputId": "6ef572e5-b9aa-4239-b982-5e2539980a7a"
      },
      "source": [
        "for i in range(1014):\n",
        "  print(train_data[0][i],end=\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23 1 12 12 70 19 20 40 70 2 5 1 18 19 70 3 12 1 23 70 2 1 3 11 70 9 14 20 15 70 20 8 5 70 2 12 1 3 11 70 64 18 5 21 20 5 18 19 65 18 5 21 20 5 18 19 70 60 70 19 8 15 18 20 60 19 5 12 12 5 18 19 38 70 23 1 12 12 70 19 20 18 5 5 20 44 19 70 4 23 9 14 4 12 9 14 7 47 2 1 14 4 70 15 6 70 21 12 20 18 1 60 3 25 14 9 3 19 38 70 1 18 5 70 19 5 5 9 14 7 70 7 18 5 5 14 70 1 7 1 9 14 40 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3FvlaCU93qS",
        "outputId": "828876b8-9f83-4f73-8f4f-6b4ff4aaabb9"
      },
      "source": [
        "test_lower[0], test_sequences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"fears for t n pension after talksunions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\",\n",
              " [6,\n",
              "  5,\n",
              "  1,\n",
              "  18,\n",
              "  19,\n",
              "  70,\n",
              "  6,\n",
              "  15,\n",
              "  18,\n",
              "  70,\n",
              "  20,\n",
              "  70,\n",
              "  14,\n",
              "  70,\n",
              "  16,\n",
              "  5,\n",
              "  14,\n",
              "  19,\n",
              "  9,\n",
              "  15,\n",
              "  14,\n",
              "  70,\n",
              "  1,\n",
              "  6,\n",
              "  20,\n",
              "  5,\n",
              "  18,\n",
              "  70,\n",
              "  20,\n",
              "  1,\n",
              "  12,\n",
              "  11,\n",
              "  19,\n",
              "  21,\n",
              "  14,\n",
              "  9,\n",
              "  15,\n",
              "  14,\n",
              "  19,\n",
              "  70,\n",
              "  18,\n",
              "  5,\n",
              "  16,\n",
              "  18,\n",
              "  5,\n",
              "  19,\n",
              "  5,\n",
              "  14,\n",
              "  20,\n",
              "  9,\n",
              "  14,\n",
              "  7,\n",
              "  70,\n",
              "  23,\n",
              "  15,\n",
              "  18,\n",
              "  11,\n",
              "  5,\n",
              "  18,\n",
              "  19,\n",
              "  70,\n",
              "  1,\n",
              "  20,\n",
              "  70,\n",
              "  20,\n",
              "  21,\n",
              "  18,\n",
              "  14,\n",
              "  5,\n",
              "  18,\n",
              "  70,\n",
              "  70,\n",
              "  70,\n",
              "  14,\n",
              "  5,\n",
              "  23,\n",
              "  1,\n",
              "  12,\n",
              "  12,\n",
              "  70,\n",
              "  19,\n",
              "  1,\n",
              "  25,\n",
              "  70,\n",
              "  20,\n",
              "  8,\n",
              "  5,\n",
              "  25,\n",
              "  70,\n",
              "  1,\n",
              "  18,\n",
              "  5,\n",
              "  70,\n",
              "  44,\n",
              "  4,\n",
              "  9,\n",
              "  19,\n",
              "  1,\n",
              "  16,\n",
              "  16,\n",
              "  15,\n",
              "  9,\n",
              "  14,\n",
              "  20,\n",
              "  5,\n",
              "  4,\n",
              "  44,\n",
              "  70,\n",
              "  1,\n",
              "  6,\n",
              "  20,\n",
              "  5,\n",
              "  18,\n",
              "  70,\n",
              "  20,\n",
              "  1,\n",
              "  12,\n",
              "  11,\n",
              "  19,\n",
              "  70,\n",
              "  23,\n",
              "  9,\n",
              "  20,\n",
              "  8,\n",
              "  70,\n",
              "  19,\n",
              "  20,\n",
              "  18,\n",
              "  9,\n",
              "  3,\n",
              "  11,\n",
              "  5,\n",
              "  14,\n",
              "  70,\n",
              "  16,\n",
              "  1,\n",
              "  18,\n",
              "  5,\n",
              "  14,\n",
              "  20,\n",
              "  70,\n",
              "  6,\n",
              "  9,\n",
              "  18,\n",
              "  13,\n",
              "  70,\n",
              "  6,\n",
              "  5,\n",
              "  4,\n",
              "  5,\n",
              "  18,\n",
              "  1,\n",
              "  12,\n",
              "  70,\n",
              "  13,\n",
              "  15,\n",
              "  7,\n",
              "  21,\n",
              "  12,\n",
              "  40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzX41jzsEZpL",
        "outputId": "ffc322bb-635c-430d-8057-eaad28dd6998"
      },
      "source": [
        "train_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmHzSjIZEdTz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}